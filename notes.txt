TODO

handle 64 bit integers on GPU side
if bcoot_64bit_word is enabled, use 64 bit integers on GPU side (if available)
this will require making 2 versions of each kernel

for functions such as extracting diagonals, there is no point in trying to do this in parallel
can implement a kernel which is instantiated only once (ie. global_work_size = 1)
which does a for-loop internally.

overall summation:
split the matrix into several even chunks.  what happens if the matrix size is odd?
or, sum each column

need to do an experiment to see if parallelisation helps
approach 1: sum each column (colwise-sum) then sum the sums (vector sum, no parallelisation)
vector sum kernel would do a for loop internally - ie. global_work_size is 1
approach 2: call vector sum directly


Mat::Mat(Mat&)        [done]
Mat::operator=(Mat&)  [done]

Mat::Mat(Mat&&)       [done]
Mat::operator=(Mat&&) [done]

need arrayops so code can be shared with Cube [done]
Mat::operator+=(Mat&) [done]
Mat::operator-=(Mat&) [done]
Mat::operator*=(Mat&) [done]
Mat::operator/=(Mat&) [done]

need arrayops so code can be shared with Cube [done]
Mat::operator= (scalar) [done]
Mat::operator+=(scalar) [done]
Mat::operator-=(scalar) [done]
Mat::operator*=(scalar) [done]
Mat::operator/=(scalar) [done]


TODO: only a small subset of element-wise operations has been implemented;
TODO: need to implement many unary math and trig functions
Mat::operator= (eOp) [done]
Mat::operator+=(eOp) [done]
Mat::operator-=(eOp) [done]
Mat::operator*=(eOp) 
Mat::operator/=(eOp) [done]

Mat::operator= (eGlue) [done]
Mat::operator+=(eGlue) [done]
Mat::operator-=(eGlue) [done]
Mat::operator*=(eGlue) 
Mat::operator/=(eGlue) [done]

Mat::operator= (Glue) 
Mat::operator+=(Glue) 
Mat::operator-=(Glue) 
Mat::operator*=(Glue) 
Mat::operator/=(Glue) 

Mat::operator= (Op) 
Mat::operator+=(Op) 
Mat::operator-=(Op) 
Mat::operator*=(Op) 
Mat::operator/=(Op) 

Mat::submat() [done]

Mat::operator= (subview) [done]
Mat::operator+=(subview) [done]
Mat::operator-=(subview) [done]
Mat::operator*=(subview) 
Mat::operator/=(subview) [done]

.col()
subview_col

.row()
subview_row

.diag()
diagview

Mat::operator= (diagview)
Mat::operator+=(diagview)
Mat::operator-=(diagview)
Mat::operator*=(diagview)
Mat::operator/=(diagview)




.is_vec()     [done]
.is_colvec()  [done]
.is_rowvec()  [done]
.is_square()  [done]
.is_empty()   [done]


glue_times -> requires Glue


op_trans
trans() -> requires Op
.t()
.st()
.ht()

.min()
.max()
.index_min()
.index_max()

as_scalar() [done]
accu()      [done]
trace()     [done]

sum(X,dim) [done]
min(X,dim)
max(X,dim)

dot(A,B)  -> how to implement this efficiently?

.zeros() [done]
.ones()  [done]
.eye()   [done]
standalone zeros()
standalone ones()
standalone eye()



randu()
randn()

all()
any()
abs()

norm()
repmat()

exp() [done]
log() [done]
clamp()
diagmat()
diagvec()
vectorise()



TODO: get a list of most used functions used by mlpack

